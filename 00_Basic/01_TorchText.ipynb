{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TorchText Github: https://github.com/pytorch/text\n",
    "* TorchText Documentation: http://torchtext.readthedocs.io/en/latest/data.html\n",
    "\n",
    "Install: \n",
    "```\n",
    "install: pip install torchtext\n",
    "```\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "* Allen Nie's article about TorchText: [A Tutorial on Torchtext](http://anie.me/On-Torchtext/)\n",
    "* yunjey's pytorch tutorial: [link](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. **Natural Language Processing Process**\n",
    "2. **Using TorchText**\n",
    "3. **What if i don't want to use TorchText**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Natural Language Processing Process\n",
    "\n",
    "For Natural language processing in Deep Learning, you will always have to do under process. \n",
    "\n",
    "1. Tokenization\n",
    "2. Build Vocabulary\n",
    "3. Numericalize all tokens\n",
    "4. Put processed data into your model: For example, input > embedding look up > rnn > output\n",
    "\n",
    "TorchText is a powerful tool for doing 1~3 process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using TorchText\n",
    "\n",
    "1. Create Field\n",
    "2. Create Datasets\n",
    "3. Build vocabulary\n",
    "4. Create Iterator\n",
    "\n",
    "Get an example from sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Iterator, TabularDataset\n",
    "with open('./data/examples.tsv') as file:\n",
    "    data = file.read().splitlines()\n",
    "    data = [line.split('\\t') for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Importance of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n",
       " '3']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create Field: \n",
    "\n",
    "* http://torchtext.readthedocs.io/en/latest/data.html#fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True,  \n",
    "             use_vocab=True,\n",
    "             tokenize=str.split,  # you can define your own tokenizer\n",
    "             lower=True, \n",
    "             batch_first=True)\n",
    "LABEL = Field(sequential=False,  \n",
    "              use_vocab=False,\n",
    "              preprocessing = lambda x: int(x),  # this preprocessing is used after Tokenize and before Numericalize\n",
    "              batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create Datasets : \n",
    "\n",
    "If you have train/dev/test datasets, you can use `TabularDataset.splits` method and add `train=`, `valid=`, `test=` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(path='./data/examples.tsv', format='tsv', fields=[('text', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Build vocabulary\n",
    "\n",
    "When buiding vocabulary, TorchText prepare \"<unk\\>\" for unknown words in valid/test data & \"<pad\\>\" tokens for padding sentences to make sure all length are same in the batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 15561\n",
      "Token for \"<unk>\": 0\n",
      "Token for \"<pad>\": 1\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "print('Total vocabulary: {}'.format(len(TEXT.vocab)))\n",
    "print('Token for \"<unk>\": {}'.format(TEXT.vocab.stoi['<unk>']))\n",
    "print('Token for \"<pad>\": {}'.format(TEXT.vocab.stoi['<pad>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Create Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Iterator(train_data, \n",
    "                        batch_size=3,  # size of batches  \n",
    "                        device=None,  # if you want to use gpu, change it to \"cuda\"\n",
    "                        repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   643,    191,      4,     43,   1447,      3,   4384,    485,\n",
      "              7,    207,    892,    107,     43,     85,    408,      3,\n",
      "            376,     17,      5,   6447,  11035,     37,     98,     43,\n",
      "            199,   5859,      2,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1],\n",
      "        [     3,   4515,     51,    444,      4,   3738,     30,     94,\n",
      "            957,   3498,     59,    700,  13967,      6,   2287,   4435,\n",
      "              4,    431,     40,      3,   1201,      7,    486,   1134,\n",
      "           4120,     59,      5,    166,   1749,    547,      6,   1339,\n",
      "            144,  14759,      2],\n",
      "        [    29,      7,    195,    568,    192,     63,    229,     60,\n",
      "             17,     21,    202,    334,     18,      5,    535,     20,\n",
      "              4,     15,    628,    231,     52,      9,    303,    195,\n",
      "           6910,      8,  10136,      8,      3,   2204,   4340,      2,\n",
      "              1,      1,      1]])\n",
      "tensor([ 0,  3,  1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "print(batch.text)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all lengths of sentences are same in each batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep down , i realized the harsh reality of my situation : i would leave the theater with a lower i.q. than when i had entered . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \u001b[1;01;36m|| len of sentence: 35 \u001b[0m\n",
      "the leaping story line , shaped by director peter kosminsky into sharp slivers and cutting impressions , shows all the signs of rich detail condensed into a few evocative images and striking character traits . \u001b[1;01;36m|| len of sentence: 35 \u001b[0m\n",
      "one of these days hollywood will come up with an original idea for a teen movie , but until then there 's always these rehashes to feed to the younger generations . <pad> <pad> <pad> \u001b[1;01;36m|| len of sentence: 35 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: TEXT.vocab.itos[x]\n",
    "for b in batch.text:\n",
    "    x = list(map(f, b.tolist()))\n",
    "    print(' '.join(x), '\\033[1;01;36m|| len of sentence: {} \\033[0m'.format(len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What if i don't want to use TorchText\n",
    "\n",
    "1. Pure Python code DataLoader\n",
    "2. Custom DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Pure Python code DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/examples.tsv') as file:\n",
    "    data = file.read().splitlines()\n",
    "    data = [line.split('\\t') for line in data]\n",
    "\n",
    "# Tokenization\n",
    "sentences, labels = list(zip(*data))\n",
    "all_tokens = [s.split() for s in sentences]\n",
    "labels = [int(l) for l in labels]\n",
    "\n",
    "#Build Vocabulary\n",
    "flatten = lambda x: [tkn for s in x for tkn in s]\n",
    "unique_tokens = set(flatten(all_tokens))\n",
    "vocab_stoi = defaultdict()\n",
    "vocab_stoi['<unk>'] = 0\n",
    "vocab_stoi['<pad>'] = 1\n",
    "for i, token in enumerate(unique_tokens, 3):\n",
    "    vocab_stoi[token] = i\n",
    "vocab_itos = [t for t, i in sorted([(token, index) for token, index in vocab_stoi.items()], key=lambda x: x[1])]\n",
    "\n",
    "#Numericalize all tokens\n",
    "all_tokens_numerical = [list(map(vocab_stoi.get, s)) for s in all_tokens]\n",
    "\n",
    "# add <pad> and create batch:\n",
    "def Loader(x, y, batch_size=32, pad_idx=1):\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(x):\n",
    "        batch_text = x[sindex:eindex]\n",
    "        batch_label = y[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        max_len = max([len(s) for s in batch_text])\n",
    "        batch_text = [s+[pad_idx]*(max_len-len(s)) if len(s) < max_len else s for s in batch_text]\n",
    "        \n",
    "        yield torch.LongTensor(batch_text), torch.LongTensor(batch_label)\n",
    "        \n",
    "    if eindex >= len(x):\n",
    "        batch_text = x[sindex:]\n",
    "        batch_label = y[sindex:]\n",
    "        max_len = max([len(s) for s in batch_text])\n",
    "        batch_text = [s+[pad_idx]*(max_len-len(s)) if len(s) < max_len else s for s in batch_text]\n",
    "        \n",
    "        yield torch.LongTensor(batch_text), torch.LongTensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_text: torch.Size([32, 41])\n",
      "batch_label: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in Loader(x=all_tokens_numerical, y=labels, batch_size=32, pad_idx=1):\n",
    "    print('batch_text:', batch[0].size())\n",
    "    print('batch_label:', batch[1].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Custom DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for custom collate_fn:\n",
    "\n",
    "1. https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278\n",
    "2. https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torchdata.Dataset):\n",
    "    def __init__(self, path='./data/examples.tsv', format_='\\t', pad_idx=1):\n",
    "        self.flatten = lambda x: [tkn for s in x for tkn in s]\n",
    "        # Preprocessing\n",
    "        with open(path, 'r') as file:\n",
    "            data = file.read().splitlines()\n",
    "            data = [line.split(format_) for line in data]\n",
    "        \n",
    "        # Tokenization\n",
    "        sentences, labels = list(zip(*data))\n",
    "        all_tokens = [s.split() for s in sentences]\n",
    "        labels = [int(l) for l in labels]\n",
    "\n",
    "        #Build Vocabulary\n",
    "        flatten = lambda x: [tkn for s in x for tkn in s]\n",
    "        unique_tokens = set(flatten(all_tokens))\n",
    "        self.vocab_stoi = defaultdict()\n",
    "        self.vocab_stoi['<unk>'] = 0\n",
    "        self.vocab_stoi['<pad>'] = 1\n",
    "        for i, token in enumerate(unique_tokens, 3):\n",
    "            self.vocab_stoi[token] = i\n",
    "        self.vocab_itos = [t for t, i in sorted([(token, index) for token, index in self.vocab_stoi.items()], key=lambda x: x[1])]\n",
    "\n",
    "        #Numericalize all tokens\n",
    "        all_tokens_numerical = [list(map(self.vocab_stoi.get, s)) for s in all_tokens]\n",
    "        \n",
    "        self.x = all_tokens_numerical\n",
    "        self.y = labels\n",
    "        self.pad_idx = 1\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # return index datas\n",
    "        return [self.x[index], self.y[index]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # lengths of data\n",
    "        return len(self.x)\n",
    "\n",
    "    def custom_collate_fn(self, data):\n",
    "        \"\"\"\n",
    "        need a custom 'collate_fn' function in 'torchdata.DataLoader' for variable length of dataset\n",
    "        \"\"\"\n",
    "        texts, labels = list(zip(*data))\n",
    "        max_len = max([len(s) for s in texts])\n",
    "        texts = [s+[self.pad_idx]*(max_len-len(s)) if len(s) < max_len else s for s in texts]\n",
    "        return torch.LongTensor(texts), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_dataset = CustomDataset(path='./data/examples.tsv', format_='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torchdata.DataLoader(dataset=exam_dataset,\n",
    "                                    collate_fn=exam_dataset.custom_collate_fn,\n",
    "                                    batch_size=32, \n",
    "                                    shuffle=False, \n",
    "                                    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_text: torch.Size([32, 41])\n",
      "batch_label: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print('batch_text:', batch[0].size())\n",
    "    print('batch_label:', batch[1].size())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
