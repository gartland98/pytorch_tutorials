{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kakao Hangul Analyzer III Tutorial\n",
    "\n",
    "\n",
    "## Getting Start\n",
    "\n",
    "* How to Install: [link](https://github.com/kakao/khaiii/blob/master/doc/setup.md)\n",
    "* env: Ubuntu Server - 18.04 LTS\n",
    "* prerequisite\n",
    "    * \\>= gcc-5.4.0\n",
    "    * \\>= cmake-3.10\n",
    "\n",
    "* How to Install prerequisite\n",
    "\n",
    "> ```\n",
    "$ sudo apt-get install gcc\n",
    "$ sudo apt-get install cmake\n",
    "```\n",
    "\n",
    "* Install\n",
    "\n",
    "> ```\n",
    "$ git clone https://github.com/kakao/khaiii.git\n",
    "$ cd khaiii\n",
    "$ pip install -r requirements.txt\n",
    "$ mkdir build && cd build\n",
    "$ cmake ..\n",
    "$ make all\n",
    "$ make resource\n",
    "$ sudo make install\n",
    "$ make package_python\n",
    "$ cd package_python\n",
    "$ pip install .   # install in local env, find your python path think carefully\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import khaiii\n",
    "api = khaiii.KhaiiiApi()\n",
    "api.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"안녕하십니까? 반가워요 누구세요\"\n",
    "sentence2 = \"hi! my name is jason. \\n how are you?\\n 안녕하세요ㅋㅋ\\n 나는 제이슨 본입니다.\"\n",
    "sentences = sentence2.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x, return_len=False):\n",
    "    \"\"\"\n",
    "    input: 'khaiii.KhaiiiWord type' type\n",
    "    api: insert khaiii api\n",
    "    return_len: return sring length\n",
    "    \"\"\"\n",
    "    assert isinstance(x, khaiii.KhaiiiWord), \"must insert khaiii.KhaiiiWord type\"\n",
    "    if return_len:\n",
    "        word_parser = lambda word: [(m.lex + '/' + m.tag, m.length) for m in word.morphs] \n",
    "    else:\n",
    "        word_parser = lambda word: [m.lex + '/' + m.tag for m in word.morphs] \n",
    "    return word_parser(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕/NNG', '하/XSA', '시/EP', 'ㅂ니까/EF', '?/SF'],\n",
       " ['반갑/VA', '어요/EC'],\n",
       " ['누구/NP', '세/VCN', '요/EC']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer(word) for word in api.analyze(sentence)]  # ok works fine, to split 3 sentences by a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi/SL', '!/SF'],\n",
       " ['my/SL'],\n",
       " ['name/SL'],\n",
       " ['is/SL'],\n",
       " ['jason/SL', './SF'],\n",
       " ['how/SL'],\n",
       " ['are/SL'],\n",
       " ['you/SL', '?/SF'],\n",
       " ['안녕/NNG', '하/XSA', '세/EC', '요/IC', 'ㅋㅋ/MAG'],\n",
       " ['나/NP', '는/JX'],\n",
       " ['제이슨/NNP'],\n",
       " ['본/NNG', '이/VCP', 'ㅂ니다/EF', './SF']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer(word) for word in api.analyze(sentence2)]  # i thought it may return 5 lenght of list that contains tokens, but it not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['hi/SL', '!/SF'], ['my/SL'], ['name/SL'], ['is/SL'], ['jason/SL', './SF']],\n",
       " [['how/SL'], ['are/SL'], ['you/SL', '?/SF']],\n",
       " [['안녕/NNG', '하/XSA', '세/EC', '요/IC', 'ㅋㅋ/MAG']],\n",
       " [['나/NP', '는/JX'], ['제이슨/NNP'], ['본/NNG', '이/VCP', 'ㅂ니다/EF', './SF']]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[tokenizer(word) for word in api.analyze(sent)] for sent in sentences]  # need to be flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[tokenizer(word) for word in api.analyze(sent)] for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda lsts: [x for lst in lsts for x in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi/SL', '!/SF', 'my/SL', 'name/SL', 'is/SL', 'jason/SL', './SF'],\n",
       " ['how/SL', 'are/SL', 'you/SL', '?/SF'],\n",
       " ['안녕/NNG', '하/XSA', '세/EC', '요/IC', 'ㅋㅋ/MAG'],\n",
       " ['나/NP', '는/JX', '제이슨/NNP', '본/NNG', '이/VCP', 'ㅂ니다/EF', './SF']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[flatten(sent) for sent in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"순결한 소년과 장미(여성)의 사랑 이야기나 갖가지 지상의 성인을 반영하는 다른 별에서 겪은 체험을 통하여 인생에 대한 일종의 초월적 비판을 시도하였다. \n",
    "그러나 이 비판을 담은 시(童心)는 그것이 비판과 분리되지 않고 일체로 되어 있기 때문에 작자의 심정과 윤리가 혼연히 융합되고 표백(表白)되어 있어, 프랑스는 물론 미국·독일 등 각국에서도 비상한 호평으로 환영하였다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순결한\t순결/NNG + 하/XSA + ㄴ/ETM\n",
      "소년과\t소년/NNG + 과/JC\n",
      "장미(여성)의\t장미/NNG + (/SS + 여성/NNG + )/SS + 의/JKG\n",
      "사랑\t사랑/NNG\n",
      "이야기나\t이야기/NNG + 나/JX\n",
      "갖가지\t갖가지/NNG\n",
      "지상의\t지상/NNG + 의/JKG\n",
      "성인을\t성인/NNG + 을/JKO\n",
      "반영하는\t반영/NNG + 하/XSV + 는/ETM\n",
      "다른\t다른/MM\n",
      "별에서\t별/NNG + 에서/JKB\n",
      "겪은\t겪/VV + 은/ETM\n",
      "체험을\t체험/NNG + 을/JKO\n",
      "통하여\t통하/VV + 여/EC\n",
      "인생에\t인생/NNG + 에/JKB\n",
      "대한\t대하/VV + ㄴ/ETM\n",
      "일종의\t일종/NNG + 의/JKG\n",
      "초월적\t초월/NNG + 적/XSN\n",
      "비판을\t비판/NNG + 을/JKO\n",
      "시도하였다.\t시도/NNG + 하/XSV + 였/EP + 다/EF + ./SF\n",
      "그러나\t그러나/MAJ\n",
      "이\t이/MM\n",
      "비판을\t비판/NNG + 을/JKO\n",
      "담은\t담/VV + 은/ETM\n",
      "시(童心)는\t시/NNG + (/SS + 童心/SH + )/SS + 는/JX\n",
      "그것이\t그것/NP + 이/JKS\n",
      "비판과\t비판/NNG + 과/JC\n",
      "분리되지\t분리/NNG + 되/XSV + 지/EC\n",
      "않고\t않/VX + 고/EC\n",
      "일체로\t일체/NNG + 로/JKB\n",
      "되어\t되/VV + 어/EC\n",
      "있기\t있/VX + 기/ETN\n",
      "때문에\t때문/NNB + 에/JKB\n",
      "작자의\t작자/NNG + 의/JKG\n",
      "심정과\t심정/NNG + 과/JC\n",
      "윤리가\t윤리/NNG + 가/JKS\n",
      "혼연히\t혼연히/MAG\n",
      "융합되고\t융합/NNG + 되/XSV + 고/EC\n",
      "표백(表白)되어\t표백/NNG + (/SS + 表白/SH + )/SS + 되/XSV + 어/EC\n",
      "있어,\t있/VX + 어/EC + ,/SP\n",
      "프랑스는\t프랑스/NNP + 는/JX\n",
      "물론\t물론/MAG\n",
      "미국·독일\t미국/NNP + ·/SP + 독일/NNP\n",
      "등\t등/NNB\n",
      "각국에서도\t각국/NNG + 에서/JKB + 도/JX\n",
      "비상한\t비상/NNG + 하/XSA + ㄴ/ETM\n",
      "호평으로\t호평/NNG + 으로/JKB\n",
      "환영하였다.\t환영/NNG + 하/XSV + 였/EP + 다/EF + ./SF\n"
     ]
    }
   ],
   "source": [
    "# return as 1 sentnecs\n",
    "for word in api.analyze(test_string):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['순결/NNG',\n",
       "  '하/XSA',\n",
       "  'ㄴ/ETM',\n",
       "  '소년/NNG',\n",
       "  '과/JC',\n",
       "  '장미/NNG',\n",
       "  '(/SS',\n",
       "  '여성/NNG',\n",
       "  ')/SS',\n",
       "  '의/JKG',\n",
       "  '사랑/NNG',\n",
       "  '이야기/NNG',\n",
       "  '나/JX',\n",
       "  '갖가지/NNG',\n",
       "  '지상/NNG',\n",
       "  '의/JKG',\n",
       "  '성인/NNG',\n",
       "  '을/JKO',\n",
       "  '반영/NNG',\n",
       "  '하/XSV',\n",
       "  '는/ETM',\n",
       "  '다른/MM',\n",
       "  '별/NNG',\n",
       "  '에서/JKB',\n",
       "  '겪/VV',\n",
       "  '은/ETM',\n",
       "  '체험/NNG',\n",
       "  '을/JKO',\n",
       "  '통하/VV',\n",
       "  '여/EC',\n",
       "  '인생/NNG',\n",
       "  '에/JKB',\n",
       "  '대하/VV',\n",
       "  'ㄴ/ETM',\n",
       "  '일종/NNG',\n",
       "  '의/JKG',\n",
       "  '초월/NNG',\n",
       "  '적/XSN',\n",
       "  '비판/NNG',\n",
       "  '을/JKO',\n",
       "  '시도/NNG',\n",
       "  '하/XSV',\n",
       "  '였/EP',\n",
       "  '다/EF',\n",
       "  './SF'],\n",
       " ['그러나/MAJ',\n",
       "  '이/MM',\n",
       "  '비판/NNG',\n",
       "  '을/JKO',\n",
       "  '담/VV',\n",
       "  '은/ETM',\n",
       "  '시/NNG',\n",
       "  '(/SS',\n",
       "  '童心/SH',\n",
       "  ')/SS',\n",
       "  '는/JX',\n",
       "  '그것/NP',\n",
       "  '이/JKS',\n",
       "  '비판/NNG',\n",
       "  '과/JC',\n",
       "  '분리/NNG',\n",
       "  '되/XSV',\n",
       "  '지/EC',\n",
       "  '않/VX',\n",
       "  '고/EC',\n",
       "  '일체/NNG',\n",
       "  '로/JKB',\n",
       "  '되/VV',\n",
       "  '어/EC',\n",
       "  '있/VX',\n",
       "  '기/ETN',\n",
       "  '때문/NNB',\n",
       "  '에/JKB',\n",
       "  '작자/NNG',\n",
       "  '의/JKG',\n",
       "  '심정/NNG',\n",
       "  '과/JC',\n",
       "  '윤리/NNG',\n",
       "  '가/JKS',\n",
       "  '혼연히/MAG',\n",
       "  '융합/NNG',\n",
       "  '되/XSV',\n",
       "  '고/EC',\n",
       "  '표백/NNG',\n",
       "  '(/SS',\n",
       "  '表白/SH',\n",
       "  ')/SS',\n",
       "  '되/XSV',\n",
       "  '어/EC',\n",
       "  '있/VX',\n",
       "  '어/EC',\n",
       "  ',/SP',\n",
       "  '프랑스/NNP',\n",
       "  '는/JX',\n",
       "  '물론/MAG',\n",
       "  '미국/NNP',\n",
       "  '·/SP',\n",
       "  '독일/NNP',\n",
       "  '등/NNB',\n",
       "  '각국/NNG',\n",
       "  '에서/JKB',\n",
       "  '도/JX',\n",
       "  '비상/NNG',\n",
       "  '하/XSA',\n",
       "  'ㄴ/ETM',\n",
       "  '호평/NNG',\n",
       "  '으로/JKB',\n",
       "  '환영/NNG',\n",
       "  '하/XSV',\n",
       "  '였/EP',\n",
       "  '다/EF',\n",
       "  './SF']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make it 2 sentnecs\n",
    "[flatten([tokenizer(word) for word in api.analyze(sent)]) for sent in test_string.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import khaiii\n",
    "import itertools\n",
    "\n",
    "class khaiii_tokenizer(khaiii.KhaiiiApi):\n",
    "    def __init__(self):\n",
    "        super(khaiii_tokenizer, self).__init__()\n",
    "        self.flatten = lambda x: list(itertools.chain.from_iterable(x))\n",
    "    \n",
    "    def morphs(self, string):\n",
    "        self.open()\n",
    "        tokens = [self.get_morphs(w) for w in self.analyze(string)]\n",
    "        self.close()\n",
    "        return self.flatten(tokens)\n",
    "    \n",
    "    def pos(self, string):\n",
    "        self.open()\n",
    "        tokens = [self.get_pos(w) for w in self.analyze(string)]\n",
    "        self.close()\n",
    "        return self.flatten(tokens)\n",
    "        \n",
    "    def get_pos(self, word):\n",
    "        assert isinstance(word, khaiii.KhaiiiWord)\n",
    "        return ['/'.join([m.lex, m.tag]) for m in word.morphs]\n",
    "    \n",
    "    def get_morphs(self, word):\n",
    "        assert isinstance(word, khaiii.KhaiiiWord)\n",
    "        return [m.lex for m in word.morphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = khaiii_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '로봇트', '는', '여기', '에', '있', '어', 'ㅋㅋ', 'ㅋ', '언제', '가', 'ㄹ까', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.morphs(\"안녕 로봇트는 여기에 있어ㅋㅋㅋ 언제 갈까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕/IC',\n",
       " '로봇트/NNG',\n",
       " '는/JX',\n",
       " '여기/NP',\n",
       " '에/JKB',\n",
       " '있/VV',\n",
       " '어/EC',\n",
       " 'ㅋㅋ/MAG',\n",
       " 'ㅋ/NNG',\n",
       " '언제/MAG',\n",
       " '가/VV',\n",
       " 'ㄹ까/EF',\n",
       " '?/SF']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pos(\"안녕 로봇트는 여기에 있어ㅋㅋㅋ 언제 갈까?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
