{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#1.-Overview)\n",
    "2. [How to represent a sentence or tokens?](#2.-How-to-represent-a-sentence-or-tokens?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Examples below are **\"Text Classification\"** problem.\n",
    "\n",
    "* Sentiment analysis: is this review positive or negative?\n",
    "* Text categorization: which category does this blog post belong to?\n",
    "* Intent classification: is this a question about a Chinese restaurant?\n",
    "\n",
    "We can define \"Text Classification\" as:\n",
    "* Input: A natural language sentence / paragraph\n",
    "* Output: a category to which the input text belongs\n",
    "    - There are a fixed number **C** of categories\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to represent a sentence or tokens?\n",
    "\n",
    "What is a sentence? A sentence can be viewed as a variable-length sequence of tokens. Each token could be any one from a vocabulary.\n",
    "\n",
    "It's very similar to talking. For example, \"the quick brown fox jumps over the lazy dog\" can be viewed as a list \"\\[the, quick, brown, fox, jumps, over, the, lazy, dog\\]\" that you speak. \n",
    "\n",
    "$$X=(x_1, x_2, \\cdots, x_t ,\\cdots,x_T) \\quad where\\ x_t \\in V$$ \n",
    "\n",
    "So, a sentence $X$ becomes a sequence of tokens, and $V$ is vocabulary set which are total unique tokens in the training data. \n",
    "\n",
    "The unit of token is not matter, you can try units like words, strings, or even digit bits. Here we take \"word\" as our token unit. \n",
    "\n",
    "Since computer machine can't understand \"words\", so we change words to an integer indices. \n",
    "\n",
    "After encoded, this sentence can be representated as **a sequence of integer indicies**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 0, 6, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "vocab = {}\n",
    "for token in sentence:\n",
    "    if vocab.get(token) is None:\n",
    "        vocab[token] = len(vocab)\n",
    "sentence = list(map(vocab.get, sentence))\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other method to represent a token is called **\"one-hot representaion\"**. Every token has a length of vocabulary size and only one of the elements is 1 at vocabulary index.\n",
    "\n",
    "For example: The token \"the\" has index 0 in our vocabulary. So, \n",
    "\n",
    "$$the = [1, 0, 0, 0, 0, 0, 0, 0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of \"the\" in the vocabulary: 0\n",
      "one-hot vector: [1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "one_hot = np.eye(len(vocab), dtype=np.int)\n",
    "print('index of \"the\" in the vocabulary:', vocab['the'])\n",
    "print('one-hot vector:', one_hot[vocab['the']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However these integer indicies are arbitrary and they can't capture the \"meaning\" of words. \n",
    "\n",
    "**What is the \"meaning(semantic)\" of a word?**\n",
    "\n",
    "There are some hypotheses in Natural Language Processing. From the paper [\"From Frequency to Meaning: Vector Space Models of Semantics\"](https://arxiv.org/abs/1003.1141)\n",
    "\n",
    ">**Statistical semantics hypothesis**: Statistical patterns of human word usage can be\n",
    "used to figure out what people mean (Weaver, 1955; Furnas et al., 1983). – If units of text\n",
    "have similar vectors in a text frequency matrix, then they tend to have similar meanings.\n",
    "(We take this to be a general hypothesis that subsumes the four more specific hypotheses\n",
    "that follow.)\n",
    ">\n",
    ">**Bag of words hypothesis**: The frequencies of words in a document tend to indicate\n",
    "the relevance of the document to a query (Salton et al., 1975). – If documents and pseudodocuments\n",
    "(queries) have similar column vectors in a term–document matrix, then they tend to have similar meanings.\n",
    ">\n",
    ">**Distributional hypothesis**: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). – If words have similar row vectors in a word–context matrix, then they tend to have similar meanings.\n",
    ">\n",
    ">**Extended distributional hypothesis**: Patterns that co-occur with similar pairs tend\n",
    "to have similar meanings (Lin & Pantel, 2001). – If patterns have similar column vectors\n",
    "in a pair–pattern matrix, then they tend to express similar semantic relations.\n",
    ">\n",
    ">**Latent relation hypothesis**: Pairs of words that co-occur in similar patterns tend\n",
    "to have similar semantic relations (Turney et al., 2003). – If word pairs have similar row\n",
    "vectors in a pair–pattern matrix, then they tend to have similar semantic relations.\n",
    "\n",
    "According to these hypotheses, we can know that the meaning of a word can be represented as a vector. Also, Similar vectors have similar meanings which means the distance between two vectors will be closer. **Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. So, we use it to measure the similarity of the meaning between two words.\n",
    "\n",
    "However, all one hot representations have same distance \"0\" when using cosine similiarity to measure their distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similiarity(x, y):\n",
    "    return (np.dot(x, y) / (np.linalg.norm(x)*np.linalg.norm(y))).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similiarity between word \"fox\" and word \"dog\" is : 0.0\n"
     ]
    }
   ],
   "source": [
    "word1 = one_hot[vocab['fox']]  # array([0, 0, 0, 1, 0, 0, 0, 0])\n",
    "word2 = one_hot[vocab['dog']]  # array([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "print('cosine similiarity between word \"fox\" and word \"dog\" is :', cos_similiarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, how should we represent a token so that it reflects its \"meaning\"? If we want to calculate some similiarity, we need to use **dense vectors** to represent these words. One-hot vectors are **sparse vectors** which contains a lot of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similiarity between \"vec1\" and \"vec2\" is : 0.994\n"
     ]
    }
   ],
   "source": [
    "vec1 = np.array([1, 2, 3, 4])\n",
    "vec2 = np.array([1, 2, 3, 5])\n",
    "print('cosine similiarity between \"vec1\" and \"vec2\" is :', cos_similiarity(vec1, vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to slove this? If we assume there is a vector sapce ($\\Bbb{R}^{\\vert V \\vert \\times d} $) that can represent these tokens. Then train this vector space in neural network to solve a classification problem. This vector space will capture the token's meaning. This process can be done by a simple matrix multiplication. Let's call word representaion vector space as $W$ which the size is $(\\vert V \\vert, d)$, since we changed tokens to one hot vectors, by doing matrix multiplication, can get a single row vector of $W$$(w_i)$ and this vector will be the meaning of the word.\n",
    "\n",
    "$$w_i = t_i \\cdot W , \\quad where\\ t_i = [0, \\cdots, \\underset{i\\text{-}th\\ index}{1}, \\cdots, 0],\\ len(t_i)=\\vert V \\vert$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector space for all tokens, size of (len(vocab), d)\n",
      "[[0.153 0.302 0.062 0.46  0.835]\n",
      " [0.927 0.727 0.768 0.269 0.644]\n",
      " [0.093 0.08  0.59  0.343 0.989]\n",
      " [0.626 0.682 0.552 0.269 0.373]\n",
      " [0.223 0.186 0.391 0.193 0.611]\n",
      " [0.883 0.622 0.253 0.18  0.816]\n",
      " [0.225 0.517 0.518 0.6   0.533]\n",
      " [0.013 0.524 0.896 0.77  0.123]]\n",
      "\n",
      "a dense vector representation for word \"fox\", at column index 3\n",
      "[0.626 0.682 0.552 0.269 0.373]\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "W = np.random.rand(len(vocab), d).round(3)\n",
    "print('vector space for all tokens, size of (len(vocab), d)')\n",
    "print(W)\n",
    "print()\n",
    "print('a dense vector representation for word \"fox\", at column index 3')\n",
    "print(np.dot(word1, W))  # one-hot token of word \"fox\" = array([0, 0, 0, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical, we just get the index of row vector of vector space $W$. At backward time, update the index of previous gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for word \"fox\" and \"dog\" is [3, 7]\n",
      "Row vector for word \"fox\" and \"dog\": \n",
      "[[0.626 0.682 0.552 0.269 0.373]\n",
      " [0.013 0.524 0.896 0.77  0.123]]\n"
     ]
    }
   ],
   "source": [
    "idxes = [np.argmax(v) for v in [word1, word2]]\n",
    "print('Index for word \"fox\" and \"dog\" is {}'.format(idxes))\n",
    "print('Row vector for word \"fox\" and \"dog\": ')\n",
    "print(W[idxes, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update word \"fox\" & \"dog\": \n",
      "[[0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.296 0.612 0.726 0.463 0.769]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.192 0.558 0.551 0.472 0.792]]\n"
     ]
    }
   ],
   "source": [
    "dout = np.random.rand(2, d).round(3)\n",
    "dW = np.zeros_like(W)\n",
    "dW[idxes] = dout\n",
    "print('Update word \"fox\" & \"dog\": ')\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for same indices, all vector refer to same indices will be summed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxes = [np.argmax(v) for v in [word1, word2, word1]]\n",
    "dout = dout[np.array([0, 1, 0])]\n",
    "temp = {i: a for i, a in zip(idxes, dout)}\n",
    "dW = np.zeros_like(W)\n",
    "for i in idxes:\n",
    "    dW[i] += 1\n",
    "for i, v in temp.items():\n",
    "    dW[i] = dW[i] * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update word \"fox\" & \"dog\": \n",
      "[[0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.592 1.224 1.452 0.926 1.538]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.192 0.558 0.551 0.472 0.792]]\n"
     ]
    }
   ],
   "source": [
    "print('Update word \"fox\" & \"dog\": ')\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is called **Embedding**. In pytorch, use \"Embedding\" Layer for embedding words to dense vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer parameters: \n",
      "Parameter containing:\n",
      "tensor([[0.1530, 0.3020, 0.0620, 0.4600, 0.8350],\n",
      "        [0.9270, 0.7270, 0.7680, 0.2690, 0.6440],\n",
      "        [0.0930, 0.0800, 0.5900, 0.3430, 0.9890],\n",
      "        [0.6260, 0.6820, 0.5520, 0.2690, 0.3730],\n",
      "        [0.2230, 0.1860, 0.3910, 0.1930, 0.6110],\n",
      "        [0.8830, 0.6220, 0.2530, 0.1800, 0.8160],\n",
      "        [0.2250, 0.5170, 0.5180, 0.6000, 0.5330],\n",
      "        [0.0130, 0.5240, 0.8960, 0.7700, 0.1230]], requires_grad=True)\n",
      "\n",
      "Embeded vector for \"fox\" and \"dog\": \n",
      "tensor([[0.6260, 0.6820, 0.5520, 0.2690, 0.3730],\n",
      "        [0.0130, 0.5240, 0.8960, 0.7700, 0.1230],\n",
      "        [0.6260, 0.6820, 0.5520, 0.2690, 0.3730]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# you can set your pretrained vector parameters giving \"_weight\" args by \"torch.Tensor\" type\n",
    "embed_layer = nn.Embedding(len(vocab), d, _weight=torch.FloatTensor(W))  \n",
    "idxes_tensor = torch.LongTensor(idxes)\n",
    "embeded = embed_layer(idxes_tensor)\n",
    "print('Embedding Layer parameters: ')\n",
    "print(embed_layer.weight)\n",
    "print()\n",
    "print('Embeded vector for \"fox\" and \"dog\": ')\n",
    "print(embeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient before backward into Embedding Layer: \n",
      "tensor([[0.2960, 0.6120, 0.7260, 0.4630, 0.7690],\n",
      "        [0.1920, 0.5580, 0.5510, 0.4720, 0.7920],\n",
      "        [0.2960, 0.6120, 0.7260, 0.4630, 0.7690]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "Update word \"fox\" & \"dog\": \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5920, 1.2240, 1.4520, 0.9260, 1.5380],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1920, 0.5580, 0.5510, 0.4720, 0.7920]],\n",
      "       dtype=torch.float64, grad_fn=<Error>)\n"
     ]
    }
   ],
   "source": [
    "dout_tensor = torch.tensor(dout, requires_grad=True)\n",
    "print('Gradient before backward into Embedding Layer: ')\n",
    "print(dout_tensor)\n",
    "print()\n",
    "print('Update word \"fox\" & \"dog\": ')\n",
    "print(embeded.grad_fn(dout_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very Easy to use, and after training, we can use word vectors in Embedding layer to calculate some similarity of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate similarity for \"fox\" & \"dog\" 0.7316\n"
     ]
    }
   ],
   "source": [
    "wv1 = embed_layer.weight[vocab.get('fox')]\n",
    "wv2 = embed_layer.weight[vocab.get('dog')]\n",
    "print('Calculate similarity for \"fox\" & \"dog\" {:.4f}'.format(F.cosine_similarity(wv1, wv2, dim=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
