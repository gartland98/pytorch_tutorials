{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Cross Entropy Loss with Softmax](#1.-Cross-Entropy-Loss-with-Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross Entropy Loss with Softmax\n",
    "In classification problem, we use cross entropy as loss function, but why do we use **cross entropy loss**?\n",
    "\n",
    "Before this let's talk about entropy and cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy & Cross Entropy\n",
    "\n",
    "In information theory, we can think [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) measures how much amout of **\"uncertainty information\"** in the data. So, when entropy is low, it means lower \"uncertainty information\" in the data, vice versa, higher \"uncertainty information\".\n",
    "\n",
    "$$H(x) = -\\sum_x p(x) \\log_2 p(x) $$\n",
    "\n",
    "For example, when we toss a coin with  known, not necessarily fair, probabilities of coming up heads or tails. When the probability of coming up head equals to 0.5, entropy is 1. This means a lot of \"uncertainty information\" in this probability distribution(which is maximum in this problem). If this probability goes up, entropy becomes lower and lower. At extreme case, when probability equals to 1, entropy becomes 0, which means no more \"uncertainty information\", because when we toss this coin, it will always show head side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_entropy(p):\n",
    "    q = 1 - p\n",
    "    if p == 0 or q == 0:\n",
    "        return 0.0\n",
    "    return - p*np.log2(p) - q*np.log2(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when head prob = 0.5, entropy = 1.0\n",
      "when head prob = 0.7, entropy = 0.881\n",
      "when head prob = 1.0, entropy = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('when head prob = 0.5, entropy =', coin_entropy(p=0.5).round(3))\n",
    "print('when head prob = 0.7, entropy =', coin_entropy(p=0.7).round(3))\n",
    "print('when head prob = 1.0, entropy =', coin_entropy(p=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we define [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two probability distributions $P$ and $Q$ to measure unknown distribution $P$ using $Q$.\n",
    "\n",
    "$$H(P, Q) = -\\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "from the definiation of entropy, we can get following equation.\n",
    "\n",
    "$$\\begin{aligned} H(P, Q) &= -\\sum_x \\sum_y p(x, y) \\log_2 p(x, y) \\\\\n",
    "&= -\\sum_x \\sum_y p(x, y) \\log_2 p(x \\vert y) p(y) \\\\\n",
    "&= -\\sum_x \\sum_y p(x, y) \\big( \\log_2 p(x \\vert y) + \\log_2 p(y) \\big)\\\\\n",
    "&= -\\sum_x \\sum_y p(x, y) \\log_2 p(x \\vert y) - \\sum_x \\sum_y p(x, y) \\log_2 p(y) \\\\\n",
    "&= -\\sum_x \\sum_y p(x, y) \\log_2 p(x \\vert y) - \\sum_y p(y) \\log_2 p(y) \\\\\n",
    "&= H(P \\vert Q) + H(Q)\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $H(P \\vert Q)$ is [conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig](./figs/0719fig_entropy.png)\n",
    "\n",
    "figure from: https://en.wikipedia.org/wiki/Conditional_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this figure and equations, two distribution will be similar when the cross entropy gets lower. Which means $H(P, Q) \\downarrow = H(P \\vert Q) \\downarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A helpful article that you can read(korean): http://sanghyukchun.github.io/62/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, why do we use cross entropy loss?\n",
    "\n",
    "Let's see following example, there are 3 classes in this problem. We assume the situation that already put a single batch input data into a model and got a output vector.\n",
    "\n",
    "Also, the target class is **1**, we encode it as one hot vector. We can think this one-hot target vector as a distribution of the calss equals to 1 in real world. Let's call it as $p(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([1, 0, 0])\n",
    "output = np.array([2.7, -8.9, 5.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we will use **softmax** function to transform this array to a probability distribution. Let's call this **output distribution ** as  $q(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)  # to avoid overflow\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.057, 0.   , 0.943])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_prob = softmax(output).round(3)\n",
    "output_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out goal is that make output distribution $q(x)$ as similar as real word distribution $p(x)$ possible. So, when we put an input to our model, it returns a right class distribution. How can we do this?\n",
    "\n",
    "Here we bring a concept called [**Kullback-Leibler divergence**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (also called **relative entropy**) . It's a measure of how one probability distribution diverges from a second, expected probability distribution. It looks like distance metric but dont forget it's not same as distance metric, since it's unsymmetric.\n",
    "\n",
    "$$D_{KL}(P \\vert\\vert Q) = - \\sum_x p(x) \\log \\dfrac{p(x)}{q(x)} = \\sum_x p(x) \\log \\dfrac{p(x)}{q(x)}$$\n",
    "\n",
    "**KL-divergence** is equal to following equation.\n",
    "\n",
    "$$\\begin{aligned} D_{KL}(P \\vert\\vert Q) &= \\sum_x p(x) \\log \\dfrac{p(x)}{q(x)} \\\\\n",
    "&= \\sum_x p(x) \\big( \\log p(x) - \\log q(x) \\big) \\\\\n",
    "&= - \\sum_x p(x) \\log q(x) + \\sum_x p(x) \\log p(x)  \\\\ \n",
    "&= H(P, Q) - H(P)\\\\ \n",
    "\\end{aligned}$$\n",
    "\n",
    "So we use KL-divergence as our loss function. To make predict distribution $Q$ closer to target distribution $P$, we need to make KL-divergence value become lower, which is equals to make corss entropy $H(P, Q)$ lower.\n",
    "\n",
    "For this example, the model output predicted class 3 as an answer(distribution=$[0.057, 0.000, 0.943]$), however our target distribution tells us the right answer is class (distribution=$[1, 0, 0]$), so the entropy will be high.\n",
    "\n",
    "$$\\begin{aligned} D_{KL} &=- \\big(1 \\times \\log(0.057) + 0 \\times \\log(0.0) + 0 \\times \\log(0.943) \\big) + \\big(1 \\times \\log(1) + 0 \\times \\log(0) + 0 \\times \\log(0) \\big) \\\\\n",
    "&= -\\log(0.057)\\end{aligned}$$\n",
    "\n",
    "As we can see, if targer vector is one hot encoded, $H(P)=0$, then cross entropy is equal to our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_with_softmax(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    if t.size == y.size:  # if target is one hot encoded it is same as get max index of batch vectors\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-99)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.864704011147587"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error_with_softmax(output_prob, target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.864704011147587"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error_with_softmax(output_prob, np.array([0]))  # same as above target index is 0, target vector = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between KL-divergence & Cross Entropy\n",
    "\n",
    "Note that the reason why we use can Cross entropy loss as our loss function, is that we encode target to one-hot vectors that makes $H(P)=0$ in **KL-divergence**. \n",
    "\n",
    "The difference: $D_{KL}(P \\vert \\vert Q)$ measures the average number of **extra** bits per message, whereas $H(P,Q)$ measures the average number of **total** bits per message.\n",
    "\n",
    "* Reference: https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Cross entropy Loss\n",
    "\n",
    "In Pytorch we don't need to calcualate probability with softmax, `CrossEntropyLoss` contains both softmax and cross entropy. Also, don't need to encode target to one hot vectors. Use LongTensor instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8590)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.FloatTensor(output).view(1, -1)\n",
    "target = torch.LongTensor([0])\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation in cross entropy loss with softmax\n",
    "\n",
    "$$\\begin{cases} \\hat{y}=\\dfrac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\\\ L=H(y, \\hat{y})=-\\sum_j y_j \\log \\hat{y}_j \\end{cases}$$\n",
    "\n",
    "where $z_i$ is linear output before softmax layer. $y$ is onehot encoded\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial \\hat{y}_i} = -y_i \\dfrac{1}{\\hat{y}_i} \\quad\\quad since\\ y_j = 0 ,i \\neq j \\\\\n",
    "\\begin{aligned} \\dfrac{\\partial \\hat{y}_i}{\\partial z_i} &= \\dfrac{e^{z_i} \\cdot \\sum_k e^{z_k} - e^{z_i} \\cdot \\dfrac{\\partial}{\\partial z_i}\\sum_k e^{z_k}}{(\\sum_k e^{z_k})^2} \\\\\n",
    "&= \\dfrac{e^{z_i}}{\\sum_k e^{z_k}} - \\dfrac{e^{z_i} \\cdot e^{z_i}}{(\\sum_k e^{z_k})^2} \\\\\n",
    "&= \\dfrac{e^{z_i}}{\\sum_k e^{z_k}} \\big(1- \\dfrac{e^{z_i}}{\\sum_k e^{z_k}} \\big) \\\\\n",
    "&= \\hat{y}_i ( 1- \\hat{y}_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "so, \n",
    "\n",
    "$$\\begin{aligned}\\dfrac{\\partial L}{\\partial z_i} &= \\dfrac{\\partial L}{\\partial \\hat{y}_i}\\dfrac{\\partial \\hat{y}_i}{\\partial z_i} \\\\\n",
    "&= -y_i \\dfrac{1}{\\hat{y}_i} \\times \\hat{y}_i ( 1- \\hat{y}_i) \\\\\n",
    "&= \\hat{y}_i - 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
